<!doctype html><html lang=en-us class="m-auto ocean"><head><title>Riccardo Belli</title><meta name=theme-color content><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta name=description content="Riccardo Belli's personal page and portfolio."><meta name=author content="Riccardo Belli"><meta name=generator content="aafu theme by Darshan in Hugo 0.109.0"><meta name=keywords content="Riccardo Belli"><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.15.2/css/all.css integrity=sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu crossorigin=anonymous><link rel=stylesheet href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Didact+Gothic%7CRoboto:400%7CRoboto+Mono"><link rel=stylesheet href=/css/aafu.css><link rel=stylesheet href=/css/aafu_compiled.css><script>let html=document.querySelector("html"),theme=window.localStorage.getItem("theme");theme?theme==="dark"?html.classList.add("dark"):html.classList.remove("dark"):html.classList.contains("dark")?window.localStorage.setItem("theme","dark"):(html.classList.remove("dark"),window.localStorage.setItem("theme","light")),window.onload=()=>{let e=document.querySelector(".theme-toggle");window.localStorage.getItem("theme")==="dark"?(e.classList.remove("bi-moon-fill"),e.classList.add("bi-brightness-high")):(e.classList.add("bi-moon-fill"),e.classList.remove("bi-brightness-high"));let t=document.querySelector(".accordion.active");t&&(t.nextElementSibling.style.maxHeight=t.nextElementSibling.scrollHeight+"px")},window.onresize=()=>{let e=document.querySelector(".accordion.active");e&&(e.nextElementSibling.style.maxHeight=e.nextElementSibling.scrollHeight+"px")}</script></head><body class="h-screen px-2 pb-2 pt-0 m-auto max-w-4xl flex flex-col"><header class="nav flex flex-row row p-2 mb-6 w-full border-b border-gray-300 dark:border-gray-700 justify-between"><div><a class="mx-0 no-underline p-2 rounded hover:bg-gray-200 dark:hover:bg-gray-800" href=https://www.riccardobelli.it/>Home</a>
<a class="mx-0 no-underline p-2 rounded hover:bg-gray-200 dark:hover:bg-gray-800" href=/blog>Blog</a>
<a class="mx-0 no-underline p-2 rounded hover:bg-gray-200 dark:hover:bg-gray-800" href=/projects>Projects</a>
<a class="mx-0 no-underline p-2 rounded hover:bg-gray-200 dark:hover:bg-gray-800" href=/files/Riccardo_Belli_CV.pdf>My CV</a></div><i class="fas fa-sun theme-toggle text-blue-500 hover:text-blue-700 dark:text-yellow-300 dark:hover:text-yellow-500 cursor-pointer text-lg px-2 mr-0 sm:mr-0" onclick=lightDark(this)></i></header><script>const lightDark=e=>{let t=document.querySelector("html");t.classList.contains("dark")?(t.classList.remove("dark"),e.classList.add("fa-moon"),e.classList.remove("fa-sun"),window.localStorage.setItem("theme","light")):(t.classList.add("dark"),e.classList.add("fa-sun"),e.classList.remove("fa-moon"),window.localStorage.setItem("theme","dark"))}</script><main class=grow><div class="prose prose-stone dark:prose-invert max-w-none"><div class=mb-3><h1 class=top-h1 style=font-size:2.75em>Convolutional networks, CIFAR-10 and complexity</h1><p class=mb-1>January 14, 2023</p><p>&mdash;</p></div><div class=content><script src=https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><p>Note: this article is also <a href=/projects/cnn-cifar10-compl/files/paper_ita.pdf>available in Italian as a pdf file</a>.</p><h1 id=sec:introduction>Introduction</h1><p>In solving an Image Classification task, the task is to
have to attribute to an image one of the labels among those
available. This means that, unlike many other tasks
classics of computer vision, we need only this information;
we won&rsquo;t have to tell where the object is in the image with a bounding
box or define membership classes pixel by pixel. Image,
one class among a set of predefined classes, with no overlap.
The CIFAR-10 dataset was used in this project, containing
60000 color images of size 32x32 pixels<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. One of the
peculiarities of this dataset is the fact that visually, the classes are
completely mutually exclusive; therefore, there will not be in the same
image a car and a truck, two of the categories of images contained
In the dataset. This is a widely used dataset and widely
used for benchmarking purposes of vision algorithms, created
as a subset of the <em>80 Million Tiny Images</em><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> dataset. The
classes featured are <em>airplanes, cars, birds, cats, deer, dogs, frogs,
horses, ships</em>, and <em>trucks</em>; it is also artificially balanced,
With 6000 images per class. Therefore, we will not have any problems related to
imbalances during classification.
For all the reasons just listed, it is a dataset that is widely
used as a benchmark of classification algorithms; its regular structure allows excellent repeatability of experiments and
Of comparing similar metrics across multiple architectures.
In our case, to address the classification task, we are going to
use two different architectures of convolutional networks. We will see
a first approach that will make use of <em>transfer learning</em>, going to
Use the ResNet-18 convolutional network as a feature extractor
pre-trained, and a second custom convolutional network architecture
made <em>ad hoc</em> for the problem, although there are no lines
unified guidelines for the definition of such architectures, but
just some <em>best practices</em>.
Finally, we will see how the computational resources available can
become a bottleneck very quickly in applications of
this genre.</p><figures><img style=margin-left:auto;margin-right:auto;max-width:90% src=images/cifar10.jpg><figcaption style=text-align:center>Sample of images taken from the CIFAR-10 dataset and the related
classes they belong to.</figcaption></figure><br><h1 id=sec:related_work>Related Work</h1><p>There is a great deal of literature in the area of Image Classification
via convolutional networks; a quick search on search engines
How Google Scholar returns tens of thousands of results between
publications and books<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. Some of the most
established in Computer Vision to perform tasks of
Image classification are VGGNet<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>, AlexNet<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>,
GoogLeNet<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> and ResNet<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>. The latter is the network that
we are going to use for our experiment, and it is also
the architecture that initiated the so-called &ldquo;revolution of depth&rdquo;
of convolutional networks. Although the version we are going to use is
composed of 18 layers, there are versions of ResNet that have more than a thousand
layer<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>.
One of the most recent publications, from 2019<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>, reports an accuracy of up to 99% for image classification of CIFAR-10; therefore, it can be said that this is a very thoroughly tested dataset.</p><h1 id=sec:approach>Proposed Approach</h1><p>The structure of the CIFAR-10 dataset includes 50000 training images and
10000 test images; they too are balanced across classes.
We therefore have 5000 training images and 1000 test images available for each class.</p><p><em>First approach: transfer learning, use of ResNet-18 as an extractor of
features.</em>
Our first approach will be to use a strategy of <em>transfer learning</em>; in fact, we will use a convolutional network pre-trained by the PyTorch<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> library and we will reuse it as a feature extractor. Therefore, we will not have to train
from scratch the network to find all the weights to assign to the many convolutional layers.
Thanks to libraries such as PyTorch this operation is very simple: we can directly import the network and replace the last fully connected layer with a dense layer having 10 output nodes, or as many as the number of classes that need to be discriminated. We can then fix the weights of the neurons of the convolutional layers, leaving to the optimizer only the work on the last fully connected layer.
One of the peculiarities of ResNet that makes it particularly high-performance is the introduction of the <strong>residual block</strong>, see Fig.<a href=#residual_block>1</a>.</p><figure id=residual_block><img style=margin-left:auto;margin-right:auto src=images/residual_block.png style=height:4cm><figcaption style=text-align:center>Image taken from the original ResNet paper: <span class=citation data-cites=resnet></span></figcaption></figure><p>This simply consists of forwarding the activation of a certain layer to a layer deeper in the network; as we can see in Fig. <a href=#residual_block>1</a>, the result of the activation coming from a previous layer is added to that of a more deep one. This simple expedient avoids problems with the numerical gradient in the optimization phase, and prevents the loss of information during the training of very deep networks.
We will see later that although this method gives the possibility to reuse some prior knowledge without having to each time start from scratch with the training of a convolutional network, not always transfer learning is an efficient procedure to solve an image classification task.</p><p>*Second approach: custom convolutional network. As a second method, we will use a convolutional network created specifically for the task, and trained on the dataset from scratch. Its layers are defined as follows:</p><ul><li><strong>INPUT</strong></li><li><strong>CONVOLUTIONAL 1</strong><ul><li>Number of filters: \(32\)</li><li>Kernel Size: \( 3\times3 \)</li><li>Padding: SAME</li><li>Stride: 1px</li></ul></li><li><strong>RELU</strong></li><li><strong>MAX POOLING 2D</strong></li><li><strong>CONVOLUTIONAL 2</strong><ul><li>Number of filters: \(64\)</li><li>Kernel Size: \( 3\times3 \)</li><li>Padding: SAME</li><li>Stride: 1px</li></ul></li><li><strong>RELU</strong></li><li><strong>MAX POOLING 2D</strong></li><li><strong>CONVOLUTIONAL 3</strong><ul><li>Number of filters: \(128\)</li><li>Kernel Size: \( 3\times3 \)</li><li>Padding: SAME</li><li>Stride: 1px</li></ul></li><li><strong>RELU</strong></li><li><strong>FULLY CONNECTED 1</strong><ul><li>Nodes Input: \( 8192: (8*8*128) \)</li><li>Output nodes: \(512\)</li></ul></li><li><strong>BATCH NORMALIZATION</strong></li><li><strong>RELU</strong></li><li><strong>DROPOUT</strong> (\(p=0.5\))</li><li><strong>FULLY CONNECTED 2</strong><ul><li>Input nodes: \(512\)</li><li>Output nodes: \( 10 \)</li></ul></li><li><strong>DROPOUT</strong> (\(p=0.5\))</li></ul><p>The loss function used for all trainings is the Cross Entropy Loss, also called Log Loss and, in the generic multi-class case
with \(n\) classes, is formulated as follows:
$$L=-\frac{1}{n}\sum_{i=1}^ny_{i}log(p_{i})$$ where \(p\) denotes
the <em>softmax</em> probability for class \(i\), and \(y_i\) is the <em>truth label</em>.
It is by no means the only loss function available; there are
others such as <em>hinge loss</em>, or <em>logistic loss</em> and others, but this one
function is certainly one of the most widely used; as stated in <em>Pattern
Recognition and Machine Learning</em><sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>: using the cross-entropy error function instead of the
sum-of-squares for a classification problem leads to faster training as
as well as improved generalization.
After the first two convolutional layers, we used <strong>MAX POOLING</strong> to reduce the resolution of filters and make the network
invariant to translations of the input, taking only the maximum of
one of the values of the region to which it belongs. This also increases the
<em>receptive field</em> of neurons deeper, allowing the network
To handle more semantic information at a time in subsequent filters,
creating feature maps that are progressively more complex.</p><figures><img style=margin-left:auto;margin-right:auto src=images/nn_arch.png><figcaption style=text-align:center>Isometric visualization of the architecture <em>custom
net</em> just described.</figcaption></figure><p><strong>DROPOUTS</strong> were inserted after both of the last two layers.
fully connected, just after nonlinearity; this was
The original approach of the authors who proposed this type of
layer<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>. One of the most recent research shows.
Of the advantages obtainable in using this type of layer even after
first convolutions, always after nonlinearities, although with some
much lower \(p\) values, around \(0.1-0.2\)<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>.
The purpose of this type of layer is to regularize the network,
forcing it to adapt to randomly broken connections with
probability \(p\) at each forward step (and then only during
training). This causes that at each training step the layer on
to which the dropout is applied is seen and treated as having a
Number of nodes and a different configuration than the pitch
previous. The end result is to approximate, or better,
simulate, the training of multiple networks with different architectures, in
parallel.<br>In addition, a <strong>BATCH NORMALIZATION</strong> layer was inserted after the first
fully connected layer to help the convergence and stability of the
network, through re-centering and re-scaling the layers of
input<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>. It was believed that this practice reduced the shift
Of the internal covariance of the parameters, a problem related to
to network initialization, but more recent studies show.
How the reason for the increase in performance is not due to this
normalization effect<sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>. In a publication
recently, it is shown that using a <em>clipping</em> technique of the
gradient and through some tricks on the tuning of certain
hyperparameters is made marginal the need for batch
normalization<sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>.</p><h1 id=sec:experiments>Experiments</h1><p>Four different <em>trainings</em> were performed:</p><ul><li><p>1-2) <strong>ResNet-18</strong>:</p><ul><li>Optimizer: Adam and SGD; best model chosen.</li><li>Learning rate scheduled from \(1 × 10^{-3}\) to \(8 × 10^{-5}\)</li><li>Loss: Cross Entropy</li></ul></li><li><p>3) <strong>Custom net 1</strong>:</p><ul><li>Optimizer: Adam</li><li>Learning rate fixed at \(1 × 10^{-3}\)</li><li>Loss: Cross Entropy</li></ul></li><li><p>4) <strong>Custom net 2</strong>:</p><ul><li>Optimizer: Adam</li><li>Learning rate scheduled from \(1 × 10^{-3}\) to \(8 × 10^{-5}\)</li><li>Loss: Cross Entropy</li></ul></li></ul><p>All nets were trained for a total of 30 epochs, and this
for a twofold reason: while the training time began
to become almost prohibitive for a larger number of eras,
On the other hand, it was seen that even as the eras (and the
decrease in the learning rate in cases (1) and (3)) no
benefit on the test accuracy metric. It may also have been
implemented an <em>early stopping</em> technique, but it was deemed useful to
let the training for all 30 eras complete the same
scheduled, to still verify that they are not in a situation
particular. In addition, better results could have been found by
Due to the continuous decrease in the learning rate.</p><figure id=accuracies_graph><img style=margin-left:auto;margin-right:auto src=images/accuracies_graph.png><figcaption style=text-align:center>Graph of the accuracies on the split test/train for the
architectures just discussed.</figcaption></figure><p><em>Custom network results</em>
Let us then analyze the results of this graph: as we can well
see, after about 10-15 epochs the test accuracy is already that
Ultimately, both for the classifier with learning rate scheduler and
for the one without, and is around a 79% for the network with
scheduler versus 77 percent for the network without. The difference is small, but
remains detectable, however, for all training eras of the
network, thus demonstrating the usefulness of this approach
To optimization with progressive decrease in learning rate.
Another thing to take into account is that the accuracy on the test set in
both cases goes up to 99%, so completely in
overfitting(?) regarding the test set.
<em>Transfer learning results with ResNet-18</em>
Regarding the results obtained with the second architecture, and
that is, the one that uses ResNet-18 as a feature extractor, the results
Unfortunately, they leave much to be desired. Although even in this case
a decreasing learning rate was used, the results of</p><p>accuracy almost immediately reach their maximum: accuracy on the
test set is close to 48% at most, and that on the train set reaches a
more stable value around 50 percent. Trying to investigate the problem,
I noticed that the network was pre-trained on the dataset
ImageNet<sup id=fnref1:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>. The latter consists of.
millions of images, divided into more than 20000 categories. The theory I have
so formulated is that the relatively shallow depth of the network
may have caused a bottleneck problem in the power of
representability of the network itself with respect to such a large amount of
information, but it is only a hypothesis.<br>Let us now look at confusion matrix: for the network trained with architecture
custom we will consider only the model that makes use of learning rate
progressive, as it reports better results overall.</p><figure id=cm_custom_net><img style=margin-left:auto;margin-right:auto src=images/cm_custom_net.png><figcaption style=text-align:center>Confusion matrix for the classifier trained on the network
custom.</figcaption></figure><p>As we see from Fig. <a href=#cm_custom_net>3</a>, classes are identified with a
confidence more than good; given their low resolution, it is
Interesting but predictable to see how the classes that come most
confused with each other are &ldquo;cat&rdquo; with &ldquo;dog,&rdquo; &ldquo;airplane&rdquo; with &ldquo;bird,&rdquo; and
&ldquo;ship&rdquo; (probably because of the often mostly blue background),
and &ldquo;automobile&rdquo; with &ldquo;truck,&rdquo; also probably due to some
common visual features among them, such as wheels or doors.</p><figure id=cifar10example><img style=margin-left:auto;margin-right:auto;width:100% src=images/cifar10example.png><figcaption style=text-align:center>Two examples of images in the dataset not easily
distinguishable from each other. The classes to which they belong are: airplane,
ship, dog and cat.</figcaption></figure><p>We see how in the example shown in Fig.
<a href=#cifar10example>4</a>,
not all images are significantly different from each other from the
visual point of view; some of them can easily be
classified incorrectly even by humans<sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>. This
explains the false positives outside the diagonal of this matrix.</p><figures><img style=margin-left:auto;margin-right:auto src=images/cm_resnet.png><figcaption style=text-align:center>Confusion matrix for the classifier adapted from
ResNet.</figcaption></figure><p>In the latter case, as we expected given the results of Top-1
accuracy in Fig. <a href=#accuracies_graph>2</a>, the classifier struggles much more to
Correctly identify the classes to which the various
images. One notices the same confusions discussed above but much
more pronounced, and <em>outliers</em> can also be seen: while there
we might as well wait for some images of low &ldquo;cat&rdquo;
resolution can be mistaken for images of &ldquo;dog,&rdquo; certainly
we do not expect these &ldquo;cat&rdquo; images to be identified as
<em>&ldquo;frog &ldquo;</em> with a frequency that is half that of the classifications
correct.
<em>On computational complexity</em>
I think it is important to mention the issue of the resources of
calculation: the custom convolutional network seen in this project was
Definitely not on par with the depth and performance of other networks
convolutional at the top of competitions such as ILSVRC<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>.
As we can see from Fig. <a href=#complexity>5</a>, not necessarily more layers,
and thus of parameters, automatically indicates better performance,
as it could only mean more problems during the
optimization. In any case, as the depth of the network increases, it will
can undoubtedly achieve a greater representative capacity,
something that I have not been able to experience as even just
adding an additional convolutional layer (and thus bringing the
total to four) already manifested the physical limitations of the computer in
my possession, despite owning a graphics card with 2GB of memory
dedicated video. With the configuration just seen, the use of
memory is around 1.3-1.5 GB during the training phase, with
a <em>batch size</em> of 64 images. By reducing this hyper-parameter you can
reduce memory use, although, against my expectations, in
rather marginally.</p><figure id=complexity><img style=margin-left:auto;margin-right:auto src=images/complexity.png><figcaption style=text-align:center class=markdownify>Top-1 accuracy of some convolutional networks in relation
to the amount of operations they required for the forward; the
blob size is proportional to the number of network parameters.</figure><p>Also related to the ILSVRC, we can see from Fig.
<a href=#ilsvrc>6</a> as with ResNet there
Has been a &ldquo;depth revolution&rdquo; of convolutional networks:</p><p>the number of layers has exploded, and the classification performance is
doubled. The real revolution, however, is that all this has not brought
also to an explosion of the parameters, and thus the size and
complexity of the networks, but as we can see from Fig.
<a href=#complexity>5</a> the number
of parameters and the operations required for forward remained more than
contained, or even decreased in some cases.</p><figure id=ilsvrc><img style=margin-left:auto;margin-right:auto src=images/ilsvrc.png><figcaption style=text-align:center>Winners of the ImageNet Large Scale Visual Recognition.
Challenge from 2010 to 2017. On the ordinates, the error of
accuracy Top-1.</figcaption></figure><p>Finally, we visually explore some of the <em>feature maps</em> that have been
extracted from the dataset. Also called &ldquo;activation maps,&rdquo; these indicate.
The activation response of a given filter to the passage of
An image in the convolutional network. In Fig.
<a href=#fm_airplane>7</a> we can
see the responses of feature map number 6 in the various layers, and how
the <em>edge</em> of the plane&rsquo;s wings is made more and more pronounced.</p><figure id=fm_airplane><img style=margin-left:auto;margin-right:auto src=images/fm_airplane.png><figcaption style=text-align:center>Input and feature map of an image belonging to the class
<em>airplane</em>.</figcaption></figure><p>In Fig. <a href=#fm_dog>8</a>,
we can see how the 11th feature map succeeds in removing some of the
Background elements as not informative for identification
of the <em>dog</em> class, and to maintain a high response on the contours
Of the dog&rsquo;s body and on the muzzle.</p><figure id=fm_dog><img style=margin-left:auto;margin-right:auto src=images/fm_dog.png><figcaption style=text-align:center>Input and feature map of an image belonging to the class
<em>dog</em>.</figcaption></figure><h1 id=sec:conclusions>Conclusion</h1><p>From the results, we can therefore conclude that even a network
from the shallow architecture such as the custom one discussed can achieve
more than satisfactory accuracy despite its simple nature.
In addition, the increase in complexity and computational resources have
quickly detected an obstacle to experimentation, failing to
To go beyond just 3 convolutional layers.<br>As for the network obtained from pre-trained ResNet, unfortunately.
performance is disappointing. As discussed earlier, the weights of the
Not fully connected layers of this network are derived from a
training on a different dataset from the one used for the rest
of the experiment, namely ImageNet<sup id=fnref2:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>. Only the last
dense layer was retrained to fit the task; however,
this approach has not proved productive. Nevertheless, this
absolutely does not mean that the <em>transfer technique is not useful
learning</em>; instead, it means that it should be used with more care, as
e.g. with a <em>fine tuning</em> of the parameters instead of keeping them
frozen; in other words, starting from a minimum, instead of remaining
There you go toward a better minimum through multiple steps of
optimization.<br>Finally, it is worth mentioning how the classes that put the most in
difficulties the network are, of course, those that are most visually close, such as
e.g. <em>cat</em> and <em>dog</em>, <em>car</em> and <em>truck</em>, and for a matter of
common background, even <em>airplane</em> is confused a significant number
of times with images of the <em>bird</em> and <em>ship</em> classes; an example is that
shown in Fig. <a href=#cifar10example>4</a>.<br>Incremental work on these results would be very easy to
realize conceptually, and could include increasing the layers
convolutional, the removal of one of the fully connected layers, and a
Better <em>fine tuning</em> of parameters in the case of reuse of
<em>transfer learning</em>. In practice, this would require resources
additional hardware, given what was discussed above.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><em>The CIFAR-10 dataset</em><br><small>Alex Krizhevsky</small>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><em>80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition</em><br><small>W. T. Freeman and R. Fergus and A. Torralba</small>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><em>convolutional neural network image classification - Google Scholar</em><br><small></small>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p><em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em><br><small>Karen Simonyan and Andrew Zisserman</small>&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p><em>Imagenet classification with deep convolutional neural networks</em><br><small>Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E</small>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p><em>Going Deeper with Convolutions</em><br><small>Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich</small>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p><em>Deep Residual Learning for Image Recognition</em><br><small>Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun</small>&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p><em>Identity Mappings in Deep Residual Networks</em><br><small>Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun</small>&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p><em>GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</em><br><small>Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen</small>&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p><em>Torchvision Models</em><br><small>The Torchvision library authors and community</small>&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p><em>Pattern Recognition and Machine Learning (Information Science and Statistics)</em><br><small>Bishop, Christopher M.</small>&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p><em>Improving neural networks by preventing co-adaptation of feature detectors</em><br><small>Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov</small>&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p><em>Analysis on the Dropout Effect in Convolutional Neural Networks</em><br><small>Park, Sungheon and Kwak, Nojun</small>&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p><em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em><br><small>Sergey Ioffe and Christian Szegedy</small>&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p><em>How Does Batch Normalization Help Optimization?</em><br><small>Shibani Santurkar and Dimitris Tsipras and Andrew Ilyas and Aleksander Madry</small>&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p><em>High-Performance Large-Scale Image Recognition Without Normalization</em><br><small>Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan</small>&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p><em>Imagenet: A large-scale hierarchical image database</em><br><small>Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li</small>&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p><em>After asking her to classify the pictures of the <a href=#cifar10example>figure</a>, a friend of mine mistakenly labeled the last two pictures as &ldquo;cat&rdquo; and &ldquo;dog&rdquo; respectively, thus reversing the classes.</em><br><small></small>&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p><em>ImageNet Large Scale Visual Recognition Challenge</em><br><small>Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei</small>&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div><div class="flex flex-row justify-around my-2"><h3 class="mb-1 mt-1 text-left mr-4"><i class="text-gray-300 dark:text-gray-600 fas fa-chevron-circle-left"></i></h3><h3 class="mb-1 mt-1 text-left ml-4"><a href=/projects/python-can-cando/ title=python-can-cando><i class="nav-menu fas fa-chevron-circle-right"></i></a></h3></div></main><footer class="text-sm text-center border-t border-gray-300 dark:border-gray-700 py-6"><p class=markdownify>Made with ❤️ and <a href=https://gohugo.io/>Hugo</a></p></footer></body></html>